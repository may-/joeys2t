{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebe9cd4b-3d98-40fc-8335-10e09a843fc1",
   "metadata": {},
   "source": [
    "# Benchmarks\n",
    "\n",
    "For the sake of reproducibility, we discribe here the detailed evaluation steps reported in the EMNLP 2022 demo submission: https://arxiv.org/abs/2210.02545\n",
    "\n",
    "dependecies:\n",
    "- joeynmt\n",
    "- datasets\n",
    "- transformers\n",
    "- pytorch\n",
    "- numpy\n",
    "- sacrebleu\n",
    "- editdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a218ef-a4c6-41e5-8fce-6c30b8bae02d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c7507ce9-ac85-439e-8c14-015ee9aa4997",
   "metadata": {},
   "source": [
    "> **Note:**:\n",
    "> We normalized the ASR transcriptions for training only, not for testing. (If we had normalized the trg text in test set too, the scores would be better.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a991fc-1f0e-403f-84ce-1b9831d4326d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07760781-dae8-44fb-8dc6-46a4c939082c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "48f6c3a6-5f83-4251-bfa8-8cd77f567f44",
   "metadata": {},
   "source": [
    "check package versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af6bc571-ada0-4f37-975b-abb3627c18e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch 1.11.0+cu115\n",
      "datasets 2.4.0\n",
      "transformers 4.21.0\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import datasets\n",
    "import transformers\n",
    "\n",
    "print('torch', torch.__version__)\n",
    "print('datasets', datasets.__version__)\n",
    "print('transformers', transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2d669c-5ce1-4788-aec3-b13bbfcf22f3",
   "metadata": {},
   "source": [
    "### metrics: WER, BLEU\n",
    "\n",
    "use joeyS2T implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fc43c4a-82f5-44b4-a5f2-1f18056aefc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joeynmt.metrics import wer, bleu\n",
    "from joeynmt.tokenizers import EvaluationTokenizer\n",
    "\n",
    "tok = EvaluationTokenizer(lowercase=True, tokenize=\"13a\", no_punc=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ba5f20-7667-4e00-85b2-bb62edf56b9b",
   "metadata": {},
   "source": [
    "## LibriSpeech Benchmarks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "700182d2-1daf-402a-9a76-a764dbd3f297",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset librispeech_asr (/workspace/mitarb/ohta/cache/librispeech_asr/all/2.1.0/14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25db8edc2ed948bfa099839d8e95faea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train.clean.100: Dataset({\n",
       "        features: ['file', 'audio', 'text', 'speaker_id', 'chapter_id', 'id'],\n",
       "        num_rows: 28539\n",
       "    })\n",
       "    train.clean.360: Dataset({\n",
       "        features: ['file', 'audio', 'text', 'speaker_id', 'chapter_id', 'id'],\n",
       "        num_rows: 104014\n",
       "    })\n",
       "    train.other.500: Dataset({\n",
       "        features: ['file', 'audio', 'text', 'speaker_id', 'chapter_id', 'id'],\n",
       "        num_rows: 148688\n",
       "    })\n",
       "    validation.clean: Dataset({\n",
       "        features: ['file', 'audio', 'text', 'speaker_id', 'chapter_id', 'id'],\n",
       "        num_rows: 2703\n",
       "    })\n",
       "    validation.other: Dataset({\n",
       "        features: ['file', 'audio', 'text', 'speaker_id', 'chapter_id', 'id'],\n",
       "        num_rows: 2864\n",
       "    })\n",
       "    test.clean: Dataset({\n",
       "        features: ['file', 'audio', 'text', 'speaker_id', 'chapter_id', 'id'],\n",
       "        num_rows: 2620\n",
       "    })\n",
       "    test.other: Dataset({\n",
       "        features: ['file', 'audio', 'text', 'speaker_id', 'chapter_id', 'id'],\n",
       "        num_rows: 2939\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "librispeech_eval = load_dataset(\"librispeech_asr\", name=\"all\")\n",
    "librispeech_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cf51e6-ef46-4746-b45a-39610f88fce8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf55e860-fc2a-4475-bbdf-f91409e53e03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "90561fa1-94cc-432b-b679-c0f9d4e70f09",
   "metadata": {},
   "source": [
    "### SpeechBrain\n",
    "\n",
    "https://huggingface.co/speechbrain/asr-transformer-transformerlm-librispeaech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df167714-e142-4a7c-84cc-b8bbd38a4e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function map_to_pred at 0x151eb61a1a20> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5501b1928abb4002ac0a16b07dd0ad3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/676 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation.clean 2.1322745487298262\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b666f6382bb940b7a587426274b72d68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/716 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation.other 5.513464709115176\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "618a1dc1be9a4fd2ad031b8f5e9fe79c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/655 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.clean 2.3128423615337796\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91ae286c181d4004af2280f334e68226",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/735 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.other 5.614886422253215\n"
     ]
    }
   ],
   "source": [
    "from speechbrain.pretrained import EncoderDecoderASR\n",
    "\n",
    "asr_model = EncoderDecoderASR.from_hparams(\n",
    "    source=\"speechbrain/asr-transformer-transformerlm-librispeech\",\n",
    "    run_opts={\"device\":\"cuda\"},\n",
    ")\n",
    "asr_model.eval()\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "def map_to_pred(batch):\n",
    "    lengths = [len(b['array']) for b in batch['audio']]\n",
    "    curr_batch_size = len(lengths)\n",
    "    max_len = max(lengths)\n",
    "    input_array = np.zeros((batch_size, max_len))\n",
    "    length_array = np.zeros((batch_size,))\n",
    "    \n",
    "    for i, b in enumerate(batch['audio']):\n",
    "        input_array[i, :lengths[i]] = b['array']\n",
    "        length_array[i] = lengths[i] / max_len\n",
    "        \n",
    "    transcription, _ = asr_model.transcribe_batch(torch.tensor(input_array), torch.tensor(length_array))\n",
    "    \n",
    "    batch[\"transcription\"] = transcription[:curr_batch_size]\n",
    "    return batch\n",
    "\n",
    "for split in ['validation.clean', 'validation.other', 'test.clean', 'test.other']:\n",
    "    result = librispeech_eval[split].map(map_to_pred,\n",
    "                                         batched=True,\n",
    "                                         batch_size=batch_size)\n",
    "                                         #remove_columns=[\"audio\"])\n",
    "    \n",
    "    print(split, wer(hypotheses=result[\"transcription\"], references=result[\"text\"], tokenizer=tok))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb03df7-b9f6-45fe-87b8-3a55c97c1abf",
   "metadata": {},
   "source": [
    "number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bef0fd8-bcb9-4a84-a07c-fd19f8e73c16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "164859096"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#model_parameters = filter(lambda p: p.requires_grad, asr_model.parameters())\n",
    "n_params = sum([np.prod(p.size()) for p in asr_model.parameters()])\n",
    "n_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8af53f7-58d1-4d35-8e4f-a78bfafa21e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f245f9db-46e2-414e-93ee-9ba4f9ada0b9",
   "metadata": {},
   "source": [
    "### facebook wav2vec2\n",
    "\n",
    "https://huggingface.co/facebook/wav2vec2-base-960h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a8c53e9-0944-4cfc-85bc-73d6eb7732a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9562380a6b2442ce967e146e7c2b2423",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2703 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation.clean 3.167162971949561\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "880b1a0b992c4fbf8a793c4c2862af25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2864 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation.other 8.860014132056214\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a89944f80e1241a79cba378c39d9f66b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2620 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.clean 3.3855751673767496\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "178678c5a5af495b8954742bd840d7a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2939 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.other 8.568480981220029\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\").to(\"cuda\")\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "model.eval()\n",
    "\n",
    "def map_to_pred(batch):\n",
    "    input_values = processor(batch[\"audio\"][\"array\"],\n",
    "                             sampling_rate=batch[\"audio\"][\"sampling_rate\"],\n",
    "                             return_tensors=\"pt\", padding=\"longest\").input_values\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_values.to(\"cuda\")).logits\n",
    "\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "    transcription = processor.batch_decode(predicted_ids)\n",
    "    batch[\"transcription\"] = transcription\n",
    "    return batch\n",
    "\n",
    "for split in ['validation.clean', 'validation.other', 'test.clean', 'test.other']:\n",
    "    result = librispeech_eval[split].map(map_to_pred,\n",
    "                                         batched=False,\n",
    "                                         batch_size=1)\n",
    "                                         #remove_columns=[\"audio\"])\n",
    "\n",
    "    hyp = [s[0] for s in result[\"transcription\"]]\n",
    "    print(split, wer(hypotheses=hyp, references=result['text'], tokenizer=tok))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9dc775-ff28-4a89-8b47-22b368fdf65b",
   "metadata": {},
   "source": [
    "number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92058553-a246-44ce-8d09-c675150b9f37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94396320"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "n_params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "n_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fab79f-d8d0-4a93-ad02-6ba165f01048",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a4d68d0b-6acd-4e7d-98b2-2992b7479f97",
   "metadata": {},
   "source": [
    "### facebook s2t\n",
    "\n",
    "https://huggingface.co/facebook/s2t-medium-librispeech-asr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27df3b34-36f9-43f2-97c0-a6191466012d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e44aa1146e4d491e81804eb10bb976d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2703 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation.clean 3.23149884195434\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78909599b7cd41ab9c9f62db6fa880b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2864 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation.other 8.008165188034859\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d389915130ed402eb1355586f5e9da26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2620 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.clean 3.5225197808886186\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c7ede9538f543628b211191f75fc58d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2939 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.other 7.832948054181074\n"
     ]
    }
   ],
   "source": [
    "from transformers import Speech2TextForConditionalGeneration, Speech2TextProcessor\n",
    "import torch\n",
    "\n",
    "model = Speech2TextForConditionalGeneration.from_pretrained(\"facebook/s2t-medium-librispeech-asr\").to(\"cuda\")\n",
    "processor = Speech2TextProcessor.from_pretrained(\"facebook/s2t-medium-librispeech-asr\", do_upper_case=False)\n",
    "model.eval()\n",
    "\n",
    "def map_to_pred(batch):\n",
    "    features = processor(batch[\"audio\"][\"array\"],\n",
    "                         sampling_rate=batch[\"audio\"][\"sampling_rate\"],\n",
    "                         padding=True, return_tensors=\"pt\")\n",
    "    input_features = features.input_features.to(\"cuda\")\n",
    "    attention_mask = features.attention_mask.to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        gen_tokens = model.generate(input_features=input_features, attention_mask=attention_mask)\n",
    "    \n",
    "    transcription = processor.batch_decode(gen_tokens, skip_special_tokens=True)\n",
    "    batch[\"transcription\"] = transcription\n",
    "    return batch\n",
    "\n",
    "for split in ['validation.clean', 'validation.other', 'test.clean', 'test.other']:\n",
    "    result = librispeech_eval[split].map(map_to_pred,\n",
    "                                         batched=False,\n",
    "                                         batch_size=1)\n",
    "                                         #remove_columns=[\"audio\"])\n",
    "    \n",
    "    ref = [s.lower() for s in result['text']]\n",
    "    hyp = [s[0] for s in result[\"transcription\"]]\n",
    "    print(split, wer(hypotheses=hyp, references=ref, tokenizer=tok))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea0efc2-1135-47d8-b269-1760bb842929",
   "metadata": {},
   "source": [
    "number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5f9776f-d68d-4d3f-98fb-980d667d4ff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71207936"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "n_params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "n_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5571b4b8-211a-4b4f-ad4b-2a0f04b37a3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc1f0b46-c53d-43fd-96d6-e4934de56f44",
   "metadata": {},
   "source": [
    "### JoeyS2T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "123a58c5-310d-472b-8f1f-5b0130a61097",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "root_dir = Path('/workspace/mitarb/ohta/models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "69c517e9-d243-48a2-8acf-6aaa1f62e470",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached sorted indices for dataset at /workspace/mitarb/ohta/cache/librispeech_asr/all/2.1.0/14c8bffddb861b4b3a4fcdff648a56980dbb808f3fc56f5a3d56b18ee88458eb/cache-5adcea4a61262a78.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- librispeech100h ---\n",
      " clean.dev: mean=10.66 std=0.36 [11.036358957391272, 10.769824638800044, 10.179772802470497]\n",
      " other.dev: mean=23.82 std=0.34 [24.228625264976053, 23.847844861427337, 23.386590248881213]\n",
      "clean.test: mean=12.02 std=0.32 [12.334525258673159, 12.150030432136335, 11.577525867315885]\n",
      "other.test: mean=24.75 std=0.37 [25.204898458246568, 24.744473950671534, 24.310796094988824]\n",
      "\n",
      "--- librispeech960h ---\n",
      " clean.dev: mean=3.79 std=0.27 [4.145068196022205, 3.501709495974413, 3.720451453990662]\n",
      " other.dev: mean=8.84 std=0.39 [9.362487241893696, 8.436052445630839, 8.728507497840935]\n",
      "clean.test: mean=4.31 std=0.52 [5.025106512477176, 3.78119293974437, 4.123554473524042]\n",
      "other.test: mean=8.66 std=0.35 [9.132071146094034, 8.318208738513269, 8.524540053111208]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "for model in ['librispeech100h', 'librispeech960h']:\n",
    "    print('---', model, '---')\n",
    "    scores = defaultdict(list)\n",
    "    for seed in [321, 42, 987]:\n",
    "        model_dir = root_dir / f'{model}_seed{seed}'\n",
    "        for split, key in [('clean.dev', 'validation.clean'),\n",
    "                          ('other.dev', 'validation.other'),\n",
    "                          ('clean.test', 'test.clean'),\n",
    "                          ('other.test', 'test.other')]:\n",
    "            data = librispeech_eval[key].sort('id')\n",
    "            hyp = (model_dir / f'avg10_{split}').read_text().splitlines()\n",
    "            ref = [s.lower() for s in data['text']]\n",
    "            assert len(hyp) == len(ref)\n",
    "\n",
    "            score = wer(hypotheses=hyp, references=ref, tokenizer=tok)\n",
    "            scores[split].append(score)\n",
    "    for k, v in scores.items():\n",
    "        print('%10s: mean=%.2f std=%.2f %r' % (k, np.mean(v), np.std(v), v))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04340870-c6dc-41a1-ab40-e6f41c896aa3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dffb3626-cfa6-4fdd-9d8d-6330626b5bcb",
   "metadata": {},
   "source": [
    "## MuST-C v1 en-de Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61f093d-21c3-4349-a00c-5de4180de57e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b6bca0e-f5da-4d66-8ee4-1fb32a92270f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration en-de-2981737b16a98f43\n",
      "Reusing dataset mustc (/workspace/mitarb/ohta/cache/mustc/en-de-2981737b16a98f43/2.0.0/06c09e13605d29280ddf6eb3fb66314164294b532f2e7f344b6d4112acc47193)\n",
      "Using custom data configuration en-de-2981737b16a98f43\n",
      "Reusing dataset mustc (/workspace/mitarb/ohta/cache/mustc/en-de-2981737b16a98f43/2.0.0/06c09e13605d29280ddf6eb3fb66314164294b532f2e7f344b6d4112acc47193)\n",
      "Using custom data configuration en-de-2981737b16a98f43\n",
      "Reusing dataset mustc (/workspace/mitarb/ohta/cache/mustc/en-de-2981737b16a98f43/2.0.0/06c09e13605d29280ddf6eb3fb66314164294b532f2e7f344b6d4112acc47193)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    validation: Dataset({\n",
       "        features: ['client_id', 'file', 'audio', 'sentence', 'translation', 'id'],\n",
       "        num_rows: 1423\n",
       "    })\n",
       "    tst.COMMON: Dataset({\n",
       "        features: ['client_id', 'file', 'audio', 'sentence', 'translation', 'id'],\n",
       "        num_rows: 2641\n",
       "    })\n",
       "    tst.HE: Dataset({\n",
       "        features: ['client_id', 'file', 'audio', 'sentence', 'translation', 'id'],\n",
       "        num_rows: 600\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import DatasetDict, load_dataset\n",
    "\n",
    "mustc_v1 = DatasetDict()\n",
    "for split in [\"validation\", \"tst.COMMON\", \"tst.HE\"]:\n",
    "    mustc_v1[split] = load_dataset(\"mustc\",\n",
    "                                   split=split,\n",
    "                                   name=\"en-de\",\n",
    "                                   data_dir=\"data/MUSTC_v1.0\")\n",
    "mustc_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ebe01f3e-bb40-443b-a699-c9588a6d7950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: MUSTC_v1=/scratch5t/ohta/MUSTC_v1.0_new\n"
     ]
    }
   ],
   "source": [
    "%env MUSTC_v1=/scratch5t/ohta/MUSTC_v1.0_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1236c47c-a043-452e-8de6-298906aa3da6",
   "metadata": {},
   "source": [
    "## fairseq s2t\n",
    "\n",
    "https://github.com/facebookresearch/fairseq/blob/main/examples/speech_to_text/docs/mustc_example.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "65f250f2-007f-4adb-9fbf-3f1d9278f3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-01 17:15:36 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
      "2022-08-01 17:15:39 | INFO | fairseq.tasks.speech_to_text | dictionary size (spm_unigram_5000.txt): 5,000\n",
      "2022-08-01 17:15:39 | INFO | fairseq_cli.generate | loading model(s) from /scratch5t/ohta/MUSTC_v1.0_new/en-de/mustc_de_asr_transformer_s.pt\n",
      "2022-08-01 17:15:43 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}\n",
      "2022-08-01 17:15:43 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/scratch5t/ohta/MUSTC_v1.0_new/en-de/spm_unigram_5000.model'}\n",
      "2022-08-01 17:15:43 | INFO | fairseq.data.audio.speech_to_text_dataset | 'dev_asr' has 0.00% OOV\n",
      "2022-08-01 17:15:43 | INFO | fairseq.data.audio.speech_to_text_dataset | SpeechToTextDataset(split=\"dev_asr\", n_samples=1_423, prepend_tgt_lang_tag=False, shuffle=False, transforms=CompositeAudioFeatureTransform(\n",
      "    UtteranceCMVN(norm_means=True, norm_vars=True)\n",
      "), n_frames_per_step=1\n",
      "  0%|                                                   | 0/117 [00:00<?, ?it/s]2022-08-01 17:15:45 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}\n",
      "2022-08-01 17:15:45 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/scratch5t/ohta/MUSTC_v1.0_new/en-de/spm_unigram_5000.model'}\n",
      "2022-08-01 17:16:58 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
      "2022-08-01 17:16:58 | INFO | fairseq_cli.generate | Translated 1,423 sentences (38,371 tokens) in 68.7s (20.71 sentences/s, 558.42 tokens/s)\n",
      "2022-08-01 17:17:02 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
      "2022-08-01 17:17:04 | INFO | fairseq.tasks.speech_to_text | dictionary size (spm_unigram_5000.txt): 5,000\n",
      "2022-08-01 17:17:05 | INFO | fairseq_cli.generate | loading model(s) from /scratch5t/ohta/MUSTC_v1.0_new/en-de/mustc_de_asr_transformer_s.pt\n",
      "2022-08-01 17:17:08 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}\n",
      "2022-08-01 17:17:08 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/scratch5t/ohta/MUSTC_v1.0_new/en-de/spm_unigram_5000.model'}\n",
      "2022-08-01 17:17:08 | INFO | fairseq.data.audio.speech_to_text_dataset | 'tst-COMMON_asr' has 0.00% OOV\n",
      "2022-08-01 17:17:08 | INFO | fairseq.data.audio.speech_to_text_dataset | SpeechToTextDataset(split=\"tst-COMMON_asr\", n_samples=2_641, prepend_tgt_lang_tag=False, shuffle=False, transforms=CompositeAudioFeatureTransform(\n",
      "    UtteranceCMVN(norm_means=True, norm_vars=True)\n",
      "), n_frames_per_step=1\n",
      "  0%|                                                   | 0/193 [00:00<?, ?it/s]2022-08-01 17:17:10 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}\n",
      "2022-08-01 17:17:10 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/scratch5t/ohta/MUSTC_v1.0_new/en-de/spm_unigram_5000.model'}\n",
      "2022-08-01 17:19:07 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
      "2022-08-01 17:19:07 | INFO | fairseq_cli.generate | Translated 2,641 sentences (65,641 tokens) in 110.3s (23.94 sentences/s, 595.07 tokens/s)\n",
      "2022-08-01 17:19:12 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
      "2022-08-01 17:19:14 | INFO | fairseq.tasks.speech_to_text | dictionary size (spm_unigram_5000.txt): 5,000\n",
      "2022-08-01 17:19:14 | INFO | fairseq_cli.generate | loading model(s) from /scratch5t/ohta/MUSTC_v1.0_new/en-de/mustc_de_asr_transformer_s.pt\n",
      "2022-08-01 17:19:17 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}\n",
      "2022-08-01 17:19:17 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/scratch5t/ohta/MUSTC_v1.0_new/en-de/spm_unigram_5000.model'}\n",
      "2022-08-01 17:19:17 | INFO | fairseq.data.audio.speech_to_text_dataset | 'tst-HE_asr' has 0.01% OOV\n",
      "2022-08-01 17:19:17 | INFO | fairseq.data.audio.speech_to_text_dataset | SpeechToTextDataset(split=\"tst-HE_asr\", n_samples=600, prepend_tgt_lang_tag=False, shuffle=False, transforms=CompositeAudioFeatureTransform(\n",
      "    UtteranceCMVN(norm_means=True, norm_vars=True)\n",
      "), n_frames_per_step=1\n",
      "  0%|                                                    | 0/54 [00:00<?, ?it/s]2022-08-01 17:19:19 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}\n",
      "2022-08-01 17:19:19 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/scratch5t/ohta/MUSTC_v1.0_new/en-de/spm_unigram_5000.model'}\n",
      "2022-08-01 17:19:53 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
      "2022-08-01 17:19:53 | INFO | fairseq_cli.generate | Translated 600 sentences (16,405 tokens) in 31.9s (18.83 sentences/s, 514.87 tokens/s)\n",
      "2022-08-01 17:19:56 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
      "2022-08-01 17:19:58 | INFO | fairseq.tasks.speech_to_text | dictionary size (spm_unigram_8000.txt): 8,000\n",
      "2022-08-01 17:19:58 | INFO | fairseq_cli.generate | loading model(s) from /scratch5t/ohta/MUSTC_v1.0_new/en-de/mustc_de_st_transformer_s.pt\n",
      "2022-08-01 17:20:02 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}\n",
      "2022-08-01 17:20:02 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/scratch5t/ohta/MUSTC_v1.0_new/en-de/spm_unigram_8000.model'}\n",
      "2022-08-01 17:20:02 | INFO | fairseq.data.audio.speech_to_text_dataset | 'dev_st' has 0.00% OOV\n",
      "2022-08-01 17:20:02 | INFO | fairseq.data.audio.speech_to_text_dataset | SpeechToTextDataset(split=\"dev_st\", n_samples=1_423, prepend_tgt_lang_tag=False, shuffle=False, transforms=CompositeAudioFeatureTransform(\n",
      "    UtteranceCMVN(norm_means=True, norm_vars=True)\n",
      "), n_frames_per_step=1\n",
      "  0%|                                                   | 0/117 [00:00<?, ?it/s]2022-08-01 17:20:04 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}\n",
      "2022-08-01 17:20:04 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/scratch5t/ohta/MUSTC_v1.0_new/en-de/spm_unigram_8000.model'}\n",
      "2022-08-01 17:21:18 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
      "2022-08-01 17:21:18 | INFO | fairseq_cli.generate | Translated 1,423 sentences (37,135 tokens) in 70.4s (20.22 sentences/s, 527.65 tokens/s)\n",
      "2022-08-01 17:22:07 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
      "2022-08-01 17:22:10 | INFO | fairseq.tasks.speech_to_text | dictionary size (spm_unigram_8000.txt): 8,000\n",
      "2022-08-01 17:22:10 | INFO | fairseq_cli.generate | loading model(s) from /scratch5t/ohta/MUSTC_v1.0_new/en-de/mustc_de_st_transformer_s.pt\n",
      "2022-08-01 17:22:13 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}\n",
      "2022-08-01 17:22:13 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/scratch5t/ohta/MUSTC_v1.0_new/en-de/spm_unigram_8000.model'}\n",
      "2022-08-01 17:22:14 | INFO | fairseq.data.audio.speech_to_text_dataset | 'tst-COMMON_st' has 0.00% OOV\n",
      "2022-08-01 17:22:14 | INFO | fairseq.data.audio.speech_to_text_dataset | SpeechToTextDataset(split=\"tst-COMMON_st\", n_samples=2_641, prepend_tgt_lang_tag=False, shuffle=False, transforms=CompositeAudioFeatureTransform(\n",
      "    UtteranceCMVN(norm_means=True, norm_vars=True)\n",
      "), n_frames_per_step=1\n",
      "  0%|                                                   | 0/193 [00:00<?, ?it/s]2022-08-01 17:22:16 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}\n",
      "2022-08-01 17:22:16 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/scratch5t/ohta/MUSTC_v1.0_new/en-de/spm_unigram_8000.model'}\n",
      "2022-08-01 17:24:15 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
      "2022-08-01 17:24:15 | INFO | fairseq_cli.generate | Translated 2,641 sentences (64,223 tokens) in 112.2s (23.54 sentences/s, 572.46 tokens/s)\n",
      "2022-08-01 17:24:21 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
      "2022-08-01 17:24:23 | INFO | fairseq.tasks.speech_to_text | dictionary size (spm_unigram_8000.txt): 8,000\n",
      "2022-08-01 17:24:23 | INFO | fairseq_cli.generate | loading model(s) from /scratch5t/ohta/MUSTC_v1.0_new/en-de/mustc_de_st_transformer_s.pt\n",
      "2022-08-01 17:24:27 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}\n",
      "2022-08-01 17:24:27 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/scratch5t/ohta/MUSTC_v1.0_new/en-de/spm_unigram_8000.model'}\n",
      "2022-08-01 17:24:27 | INFO | fairseq.data.audio.speech_to_text_dataset | 'tst-HE_st' has 0.01% OOV\n",
      "2022-08-01 17:24:27 | INFO | fairseq.data.audio.speech_to_text_dataset | SpeechToTextDataset(split=\"tst-HE_st\", n_samples=600, prepend_tgt_lang_tag=False, shuffle=False, transforms=CompositeAudioFeatureTransform(\n",
      "    UtteranceCMVN(norm_means=True, norm_vars=True)\n",
      "), n_frames_per_step=1\n",
      "  0%|                                                    | 0/54 [00:00<?, ?it/s]2022-08-01 17:24:29 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}\n",
      "2022-08-01 17:24:29 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/scratch5t/ohta/MUSTC_v1.0_new/en-de/spm_unigram_8000.model'}\n",
      "2022-08-01 17:25:03 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
      "2022-08-01 17:25:03 | INFO | fairseq_cli.generate | Translated 600 sentences (15,681 tokens) in 32.5s (18.49 sentences/s, 483.13 tokens/s)\n"
     ]
    }
   ],
   "source": [
    "!for task in asr st; \\\n",
    "    do for split in dev tst-COMMON tst-HE; \\\n",
    "        do fairseq-generate ${MUSTC_v1}/en-de \\\n",
    "            --config-yaml config_${task}.yaml \\\n",
    "            --gen-subset ${split}_${task} \\\n",
    "            --task speech_to_text \\\n",
    "            --path ${MUSTC_v1}/en-de/mustc_de_${task}_transformer_s.pt \\\n",
    "            --max-tokens 10000 \\\n",
    "            --beam 20 > ${MUSTC_v1}/en-de/${split}_${task}.log; \\\n",
    "    done; \\\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dfd3a223-6959-42ed-8191-c4dba2385a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "!for task in asr st; \\\n",
    "    do for split in dev tst-COMMON tst-HE; \\\n",
    "        do grep ^D ${MUSTC_v1}/en-de/${split}_${task}.log | LC_ALL=C sort -V | cut -f3- > ${MUSTC_v1}/en-de/${split}_${task}.hyp; \\\n",
    "    done; \\\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "965e7079-7909-4887-9744-acd3e41def55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- asr ---\n",
      "         dev 13.07\n",
      "  tst-COMMON 12.72\n",
      "      tst-HE 10.93\n",
      "\n",
      "--- st ---\n",
      "         dev 22.05\n",
      "  tst-COMMON 22.70\n",
      "      tst-HE 21.70\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for task in ['asr', 'st']:\n",
    "    print('---', task, '---')\n",
    "    for split, key in [('dev', 'validation'), ('tst-COMMON', 'tst.COMMON'), ('tst-HE', 'tst.HE')]:\n",
    "        ref = mustc_v1[key]\n",
    "        hyp = (Path(os.environ['MUSTC_v1']) / f'en-de/{split}_{task}.hyp').read_text().splitlines()\n",
    "        assert len(ref) == len(hyp)\n",
    "        \n",
    "        if task == 'asr':\n",
    "            score = wer(hypotheses=hyp, references=ref['sentence'], tokenizer=tok)\n",
    "        elif task == 'st':\n",
    "            score = bleu(hypotheses=hyp, references=ref['translation'], tokenize=\"13a\")\n",
    "            \n",
    "        print('%12s %2.2f' % (split, score))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38455f3-ec7a-4d27-b932-ab57a8d5d586",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04e93380-51c5-490f-a4f0-b431220975c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def load_tsv(path):\n",
    "    p = path.read_text().splitlines()\n",
    "    entries = []\n",
    "    for i, line in enumerate(p):\n",
    "        if i > 0:\n",
    "            line = line.split('\\t')\n",
    "            entries.append({\n",
    "                'id': line[0],\n",
    "                'audio': line[1],\n",
    "                'n_frames': int(line[2]),\n",
    "                'tgt_text': line[3],\n",
    "                'speaker': line[4],\n",
    "            })\n",
    "    df = pd.DataFrame.from_dict(entries)\n",
    "    #df = pd.read_csv(path, sep=\"\\t\")\n",
    "    return df\n",
    "\n",
    "root_dir = Path('/workspace/mitarb/ohta/models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a4623936-d463-4698-8ed5-56a76467235e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- mustc_v2_asr ---\n",
      "         dev: mean=18.79 std=0.61 [19.57116617310792, 18.716497357274058, 18.08674138771226]\n",
      "  tst-COMMON: mean=18.86 std=0.37 [19.384788927936135, 18.538765239272852, 18.66458414681765]\n",
      "      tst-HE: mean=15.19 std=0.56 [15.956973953240288, 14.650191128100277, 14.97021957507334]\n",
      "\n",
      "--- mustc_v2_mt ---\n",
      "         dev: mean=21.65 std=0.24 [21.772558277350843, 21.85346651122046, 21.313055322761763]\n",
      "  tst-COMMON: mean=23.07 std=0.14 [23.184641282742795, 23.148724630486548, 22.87574048857216]\n",
      "      tst-HE: mean=20.21 std=0.17 [20.296177594561602, 20.372050882152354, 19.969481160302298]\n",
      "\n",
      "--- mustc_v2_mt_cascade ---\n",
      "         dev: mean=21.43 std=0.63 [20.54657959064018, 21.79377175639091, 21.95741642064011]\n",
      "  tst-COMMON: mean=21.89 std=0.64 [20.993011117357035, 22.328004351471453, 22.36151621616054]\n",
      "      tst-HE: mean=21.03 std=0.66 [20.22587289794963, 21.848313074776083, 21.013385180851117]\n",
      "\n",
      "--- mustc_v2_st ---\n",
      "         dev: mean=20.48 std=0.48 [20.494653179350077, 19.88471654327755, 21.05656309655579]\n",
      "  tst-COMMON: mean=20.53 std=0.29 [20.431242048357298, 20.229650809326206, 20.921448119571835]\n",
      "      tst-HE: mean=21.13 std=0.46 [20.75129148376672, 20.873775196314327, 21.778859229806404]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for task, t in [('asr', ''), ('mt', ''), ('mt', '_cascade'), ('st', '')]:\n",
    "    model = f'mustc_v2_{task}'\n",
    "    print('---', f'{model}{t}', '---')\n",
    "    scores = defaultdict(list)\n",
    "    for seed in [321, 42, 987]:\n",
    "        model_dir = root_dir / f'{model}_seed{seed}'\n",
    "        for split, key in [('dev', 'validation'), ('tst-COMMON', 'tst.COMMON'), ('tst-HE', 'tst.HE')]:\n",
    "            ref = mustc_v1[key]\n",
    "            ckpt = \"avg10\" if task in [\"asr\", \"st\"] else \"avg5\"\n",
    "            ext = \".en\" if task in [\"asr\"] else \"\"\n",
    "            hyp_raw = (model_dir / f'{ckpt}{t}_v1.{split}{ext}').read_text().splitlines()\n",
    "            \n",
    "            if f'{task}{t}' in [\"asr\", \"st\", \"mt_cascade\"]:\n",
    "                tt = task if task in [\"asr\", \"st\"] else \"asr\"\n",
    "                df = load_tsv(Path(os.environ['MUSTC_v1']) / f'en-de/joey_{split}_{tt}.tsv')\n",
    "                short_items = df[df['n_frames'] <= 10]\n",
    "                hyp = []\n",
    "                for item in ref:\n",
    "                    idx = item['id']\n",
    "                    if (idx not in df['id'].tolist()) or (idx in short_items['id'].tolist()):\n",
    "                        hyp.append('')\n",
    "                    else:\n",
    "                        hyp.append(hyp_raw.pop(0))\n",
    "            else:\n",
    "                hyp = hyp_raw\n",
    "            assert len(hyp) == len(ref), (len(hyp), len(ref))\n",
    "            \n",
    "            if task in [\"asr\"]:\n",
    "                score = wer(hypotheses=hyp, references=ref['sentence'], tokenizer=tok)\n",
    "                \n",
    "            elif task in [\"mt\", \"st\"]:\n",
    "                score = bleu(hypotheses=hyp, references=ref['translation'], tokenize=\"13a\")\n",
    "                \n",
    "            scores[split].append(score)\n",
    "\n",
    "    for k, v in scores.items():\n",
    "        print('%12s: mean=%.2f std=%.2f %r' % (k, np.mean(v), np.std(v), v))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0a6d2b-ae8b-4bd6-aea2-24b667597482",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fa5813-d98c-4927-8eac-96d935badb16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0386ee-6864-4293-8aa9-d5b1035cdf25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e155e4ca-bbcb-41de-9e5c-68aef0e98380",
   "metadata": {},
   "source": [
    "## MuST-C v2 en-de Benchmarks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "f60109e2-d609-4f3c-b1a6-73d38a485e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration en-de-f1a63e2b2e62995c\n",
      "Reusing dataset mustc (/workspace/mitarb/ohta/cache/mustc/en-de-f1a63e2b2e62995c/2.0.0/06c09e13605d29280ddf6eb3fb66314164294b532f2e7f344b6d4112acc47193)\n",
      "Using custom data configuration en-de-f1a63e2b2e62995c\n",
      "Reusing dataset mustc (/workspace/mitarb/ohta/cache/mustc/en-de-f1a63e2b2e62995c/2.0.0/06c09e13605d29280ddf6eb3fb66314164294b532f2e7f344b6d4112acc47193)\n",
      "Using custom data configuration en-de-f1a63e2b2e62995c\n",
      "Reusing dataset mustc (/workspace/mitarb/ohta/cache/mustc/en-de-f1a63e2b2e62995c/2.0.0/06c09e13605d29280ddf6eb3fb66314164294b532f2e7f344b6d4112acc47193)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    validation: Dataset({\n",
       "        features: ['client_id', 'file', 'audio', 'sentence', 'translation', 'id'],\n",
       "        num_rows: 1415\n",
       "    })\n",
       "    tst.COMMON: Dataset({\n",
       "        features: ['client_id', 'file', 'audio', 'sentence', 'translation', 'id'],\n",
       "        num_rows: 2580\n",
       "    })\n",
       "    tst.HE: Dataset({\n",
       "        features: ['client_id', 'file', 'audio', 'sentence', 'translation', 'id'],\n",
       "        num_rows: 600\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import DatasetDict, load_dataset\n",
    "\n",
    "mustc_v2 = DatasetDict()\n",
    "for split in [\"validation\", \"tst.COMMON\", \"tst.HE\"]:\n",
    "    mustc_v2[split] = load_dataset(\"mustc\",\n",
    "                                   split=split,\n",
    "                                   name=\"en-de\",\n",
    "                                   data_dir=\"data/MUSTC_v2.0\")\n",
    "mustc_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "5d1a59a4-9b22-4991-b6cb-9dfc65aeeb1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: MUSTC_v2=/scratch5t/ohta/MUSTC_v2.0_new\n"
     ]
    }
   ],
   "source": [
    "%env MUSTC_v2=/scratch5t/ohta/MUSTC_v2.0_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897b1c20-cbb5-467e-a413-1d39dd0e6a74",
   "metadata": {},
   "source": [
    "### fairseq s2t\n",
    "\n",
    "https://github.com/facebookresearch/fairseq/blob/main/examples/speech_to_text/docs/mustc_example.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "024c5bd4-6354-4f82-ac50-6933b4ba06a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-29 18:27:15 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
      "2022-07-29 18:27:17 | INFO | fairseq.tasks.speech_to_text | dictionary size (spm_unigram_5000.txt): 5,000\n",
      "2022-07-29 18:27:17 | INFO | fairseq_cli.generate | loading model(s) from /scratch5t/ohta/MUSTC_v2.0_new/en-de/mustc_de_asr_transformer_s.pt\n",
      "2022-07-29 18:27:20 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}\n",
      "2022-07-29 18:27:20 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/scratch5t/ohta/MUSTC_v2.0_new/en-de/spm_unigram_5000.model'}\n",
      "2022-07-29 18:27:20 | INFO | fairseq.data.audio.speech_to_text_dataset | 'dev_asr' has 0.42% OOV\n",
      "2022-07-29 18:27:20 | INFO | fairseq.data.audio.speech_to_text_dataset | SpeechToTextDataset(split=\"dev_asr\", n_samples=1_415, prepend_tgt_lang_tag=False, shuffle=False, transforms=CompositeAudioFeatureTransform(\n",
      "    UtteranceCMVN(norm_means=True, norm_vars=True)\n",
      "), n_frames_per_step=1\n",
      "  0%|                                                   | 0/118 [00:00<?, ?it/s]2022-07-29 18:27:22 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}\n",
      "2022-07-29 18:27:22 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/scratch5t/ohta/MUSTC_v2.0_new/en-de/spm_unigram_5000.model'}\n",
      "2022-07-29 18:28:50 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
      "2022-07-29 18:28:50 | INFO | fairseq_cli.generate | Translated 1,415 sentences (38,685 tokens) in 83.3s (16.99 sentences/s, 464.56 tokens/s)\n",
      "2022-07-29 18:28:54 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
      "2022-07-29 18:28:56 | INFO | fairseq.tasks.speech_to_text | dictionary size (spm_unigram_5000.txt): 5,000\n",
      "2022-07-29 18:28:56 | INFO | fairseq_cli.generate | loading model(s) from /scratch5t/ohta/MUSTC_v2.0_new/en-de/mustc_de_asr_transformer_s.pt\n",
      "2022-07-29 18:28:59 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}\n",
      "2022-07-29 18:28:59 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/scratch5t/ohta/MUSTC_v2.0_new/en-de/spm_unigram_5000.model'}\n",
      "2022-07-29 18:28:59 | INFO | fairseq.data.audio.speech_to_text_dataset | 'tst-COMMON_asr' has 0.40% OOV\n",
      "2022-07-29 18:28:59 | INFO | fairseq.data.audio.speech_to_text_dataset | SpeechToTextDataset(split=\"tst-COMMON_asr\", n_samples=2_580, prepend_tgt_lang_tag=False, shuffle=False, transforms=CompositeAudioFeatureTransform(\n",
      "    UtteranceCMVN(norm_means=True, norm_vars=True)\n",
      "), n_frames_per_step=1\n",
      "  0%|                                                   | 0/193 [00:00<?, ?it/s]2022-07-29 18:29:02 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}\n",
      "2022-07-29 18:29:02 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/scratch5t/ohta/MUSTC_v2.0_new/en-de/spm_unigram_5000.model'}\n",
      " 52%|████████████████▋               | 101/193 [01:30<00:52,  1.76it/s, wps=345]2022-07-29 18:32:17 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
      "2022-07-29 18:32:19 | INFO | fairseq.tasks.speech_to_text | dictionary size (spm_unigram_8000.txt): 8,000\n",
      "2022-07-29 18:32:19 | INFO | fairseq_cli.generate | loading model(s) from /scratch5t/ohta/MUSTC_v2.0_new/en-de/mustc_de_st_transformer_s.pt\n",
      "2022-07-29 18:32:23 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}\n",
      "2022-07-29 18:32:23 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/scratch5t/ohta/MUSTC_v2.0_new/en-de/spm_unigram_8000.model'}\n",
      "2022-07-29 18:32:23 | INFO | fairseq.data.audio.speech_to_text_dataset | 'dev_st' has 0.45% OOV\n",
      "2022-07-29 18:32:23 | INFO | fairseq.data.audio.speech_to_text_dataset | SpeechToTextDataset(split=\"dev_st\", n_samples=1_415, prepend_tgt_lang_tag=False, shuffle=False, transforms=CompositeAudioFeatureTransform(\n",
      "    UtteranceCMVN(norm_means=True, norm_vars=True)\n",
      "), n_frames_per_step=1\n",
      "  0%|                                                   | 0/118 [00:00<?, ?it/s]2022-07-29 18:32:25 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}\n",
      "2022-07-29 18:32:25 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/scratch5t/ohta/MUSTC_v2.0_new/en-de/spm_unigram_8000.model'}\n",
      "2022-07-29 18:33:54 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
      "2022-07-29 18:33:54 | INFO | fairseq_cli.generate | Translated 1,415 sentences (37,652 tokens) in 84.2s (16.80 sentences/s, 447.02 tokens/s)\n",
      "2022-07-29 18:33:58 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
      "2022-07-29 18:34:00 | INFO | fairseq.tasks.speech_to_text | dictionary size (spm_unigram_8000.txt): 8,000\n",
      "2022-07-29 18:34:00 | INFO | fairseq_cli.generate | loading model(s) from /scratch5t/ohta/MUSTC_v2.0_new/en-de/mustc_de_st_transformer_s.pt\n",
      "2022-07-29 18:34:04 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}\n",
      "2022-07-29 18:34:04 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/scratch5t/ohta/MUSTC_v2.0_new/en-de/spm_unigram_8000.model'}\n",
      "2022-07-29 18:34:04 | INFO | fairseq.data.audio.speech_to_text_dataset | 'tst-COMMON_st' has 0.37% OOV\n",
      "2022-07-29 18:34:04 | INFO | fairseq.data.audio.speech_to_text_dataset | SpeechToTextDataset(split=\"tst-COMMON_st\", n_samples=2_580, prepend_tgt_lang_tag=False, shuffle=False, transforms=CompositeAudioFeatureTransform(\n",
      "    UtteranceCMVN(norm_means=True, norm_vars=True)\n",
      "), n_frames_per_step=1\n",
      "  0%|                                                   | 0/193 [00:00<?, ?it/s]2022-07-29 18:34:06 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}\n",
      "2022-07-29 18:34:06 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/scratch5t/ohta/MUSTC_v2.0_new/en-de/spm_unigram_8000.model'}\n",
      "2022-07-29 18:36:32 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
      "2022-07-29 18:36:32 | INFO | fairseq_cli.generate | Translated 2,580 sentences (64,662 tokens) in 137.5s (18.76 sentences/s, 470.17 tokens/s)\n",
      "2022-07-29 18:36:36 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
      "2022-07-29 18:36:38 | INFO | fairseq.tasks.speech_to_text | dictionary size (spm_unigram_8000.txt): 8,000\n",
      "2022-07-29 18:36:38 | INFO | fairseq_cli.generate | loading model(s) from /scratch5t/ohta/MUSTC_v2.0_new/en-de/mustc_de_st_transformer_s.pt\n",
      "2022-07-29 18:36:42 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}\n",
      "2022-07-29 18:36:42 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/scratch5t/ohta/MUSTC_v2.0_new/en-de/spm_unigram_8000.model'}\n",
      "2022-07-29 18:36:42 | INFO | fairseq.data.audio.speech_to_text_dataset | 'tst-HE_st' has 0.45% OOV\n",
      "2022-07-29 18:36:42 | INFO | fairseq.data.audio.speech_to_text_dataset | SpeechToTextDataset(split=\"tst-HE_st\", n_samples=600, prepend_tgt_lang_tag=False, shuffle=False, transforms=CompositeAudioFeatureTransform(\n",
      "    UtteranceCMVN(norm_means=True, norm_vars=True)\n",
      "), n_frames_per_step=1\n",
      "  0%|                                                    | 0/55 [00:00<?, ?it/s]2022-07-29 18:36:44 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}\n",
      "2022-07-29 18:36:44 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/scratch5t/ohta/MUSTC_v2.0_new/en-de/spm_unigram_8000.model'}\n",
      "2022-07-29 18:37:26 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
      "2022-07-29 18:37:26 | INFO | fairseq_cli.generate | Translated 600 sentences (15,917 tokens) in 39.8s (15.06 sentences/s, 399.62 tokens/s)\n"
     ]
    }
   ],
   "source": [
    "!for task in asr st; \\\n",
    "    do for split in dev tst-COMMON tst-HE; \\\n",
    "        do fairseq-generate ${MUSTC_v2}/en-de \\\n",
    "            --config-yaml config_${task}.yaml \\\n",
    "            --gen-subset ${split}_${task} \\\n",
    "            --task speech_to_text \\\n",
    "            --path ${MUSTC_v2}/en-de/mustc_de_${task}_transformer_s.pt \\\n",
    "            --max-tokens 10000 \\\n",
    "            --beam 20 > ${MUSTC_v2}/en-de/${split}_${task}.log; \\\n",
    "    done; \\\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b2211394-f435-4a04-8735-716ac8822031",
   "metadata": {},
   "outputs": [],
   "source": [
    "!for task in asr st; \\\n",
    "    do for split in dev tst-COMMON tst-HE; \\\n",
    "        do grep ^D ${MUSTC_v2}/en-de/${split}_${task}.log | LC_ALL=C sort -V | cut -f3- > ${MUSTC_v2}/en-de/${split}_${task}.hyp; \\\n",
    "    done; \\\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "71feecf9-784a-4538-b245-798acbff5376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- asr ---\n",
      "         dev 9.11\n",
      "  tst-COMMON 11.88\n",
      "      tst-HE 10.43\n",
      "\n",
      "--- st ---\n",
      "         dev 23.38\n",
      "  tst-COMMON 23.20\n",
      "      tst-HE 22.23\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for task in ['asr', 'st']:\n",
    "    print('---', task, '---')\n",
    "    for split, key in [('dev', 'validation'), ('tst-COMMON', 'tst.COMMON'), ('tst-HE', 'tst.HE')]:\n",
    "        ref = mustc_ende[key]\n",
    "        hyp = (Path(os.environ['MUSTC_v2']) / f'en-de/{split}_{task}.hyp').read_text().splitlines()\n",
    "        assert len(ref) == len(hyp)\n",
    "        \n",
    "        if task == 'asr':\n",
    "            score = wer(hypotheses=hyp, references=ref['sentence'], tokenizer=tok)\n",
    "        elif task == 'st':\n",
    "            score = bleu(hypotheses=hyp, references=ref['translation'], tokenize=\"13a\")\n",
    "            \n",
    "        print('%12s %2.2f' % (split, score))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37e0701-e321-4f91-8d2f-981318f59651",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ad47156e-32cc-4565-8867-3c48c469d1e6",
   "metadata": {},
   "source": [
    "### JoeyS2T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "3ef963fa-66e4-461c-9d14-35589a2da583",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "root_dir = Path('/workspace/mitarb/ohta/models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "df8a1de3-b42f-42d8-b252-dd2a996b4192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- mustc_v2_asr ---\n",
      "         dev: mean=10.27 std=0.43 [10.819426214138383, 10.226889180573785, 9.769360585036564]\n",
      "  tst-COMMON: mean=12.95 std=0.32 [13.276147348253941, 13.062788469912043, 12.511974222764085]\n",
      "      tst-HE: mean=11.16 std=0.31 [11.427057576827975, 11.329918756623101, 10.72942423172024]\n",
      "\n",
      "--- mustc_v2_mt ---\n",
      "         dev: mean=26.45 std=0.75 [25.390705502875505, 26.978046442438234, 26.99326579089932]\n",
      "  tst-COMMON: mean=27.17 std=0.63 [26.27450170545455, 27.612297404963513, 27.608497352365507]\n",
      "      tst-HE: mean=24.85 std=0.68 [23.890310521482185, 25.397185257314472, 25.25525286854287]\n",
      "\n",
      "--- mustc_v2_mt_cascade ---\n",
      "         dev: mean=23.86 std=0.76 [22.788992194115746, 24.377921594559243, 24.422131504399257]\n",
      "  tst-COMMON: mean=23.95 std=0.59 [23.11313444237887, 24.344195901542143, 24.400466368856243]\n",
      "      tst-HE: mean=22.65 std=0.58 [21.834181399166415, 22.99811303119327, 23.131362026298824]\n",
      "\n",
      "--- mustc_v2_st ---\n",
      "         dev: mean=23.52 std=0.53 [23.28003961548913, 23.025462738308008, 24.25843874947149]\n",
      "  tst-COMMON: mean=23.33 std=0.39 [23.200343619307045, 22.921212910802815, 23.860570775979287]\n",
      "      tst-HE: mean=22.90 std=0.69 [22.591305506751347, 22.255287057447543, 23.86011139977676]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for task, t in [('asr', ''), ('mt', ''), ('mt', '_cascade'), ('st', '')]:\n",
    "    model = f'mustc_v2_{task}'\n",
    "    print('---', f'{model}{t}', '---')\n",
    "    scores = defaultdict(list)\n",
    "    for seed in [321, 42, 987]:\n",
    "        model_dir = root_dir / f'{model}_seed{seed}'\n",
    "        for split, key in [('dev', 'validation'), ('tst-COMMON', 'tst.COMMON'), ('tst-HE', 'tst.HE')]:\n",
    "            ref = mustc_v2[key]\n",
    "            ckpt = \"avg10\" if task in [\"asr\", \"st\"] else \"avg5\"\n",
    "            ext = \".en\" if task in [\"asr\"] else \"\"\n",
    "            hyp_raw = (model_dir / f'{ckpt}{t}.{split}{ext}').read_text().splitlines()\n",
    "            \n",
    "            if f'{task}{t}' in [\"asr\", \"st\", \"mt_cascade\"]:\n",
    "                tt = task if task in [\"asr\", \"st\"] else \"asr\"\n",
    "                df = load_tsv(Path(os.environ['MUSTC_v2']) / f'en-de/joey_{split}_{tt}.tsv')\n",
    "                short_items = df[df['n_frames'] <= 10]\n",
    "                hyp = []\n",
    "                for item in ref:\n",
    "                    idx = item['id']\n",
    "                    if (idx not in df['id'].tolist()) or (idx in short_items['id'].tolist()):\n",
    "                        hyp.append('')\n",
    "                    else:\n",
    "                        hyp.append(hyp_raw.pop(0))\n",
    "            else:\n",
    "                hyp = hyp_raw\n",
    "            assert len(hyp) == len(ref), (len(hyp), len(ref))\n",
    "            \n",
    "            if task in [\"asr\"]:\n",
    "                score = wer(hypotheses=hyp, references=ref['sentence'], tokenizer=tok)\n",
    "                \n",
    "            elif task in [\"mt\", \"st\"]:\n",
    "                score = bleu(hypotheses=hyp, references=ref['translation'], tokenize=\"13a\")\n",
    "                \n",
    "            scores[split].append(score)\n",
    "\n",
    "    for k, v in scores.items():\n",
    "        print('%12s: mean=%.2f std=%.2f %r' % (k, np.mean(v), np.std(v), v))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526c331e-2fd7-40f3-85ef-4722ff30a1ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f22eac-60e4-41f6-a65e-6b60da1a68e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac3ce62-9bbd-476f-a156-43d178486245",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
