.. _benchmarks:

==========
Benchmarks
==========


We provide several pretrained models with their benchmark results.


JoeyS2T
-------


* For ASR task, we compute WER (lower is better)
* For MT and ST task, we compute BLEU (higher is better)


LibriSpeech 100h
^^^^^^^^^^^^^^^^

+------------------------------------------------------------------------+--------------+-----------+-----------+------------+------------+---------+-------------------------------------------+
| System                                                                 | Architecture | dev-clean | dev-other | test-clean | test-other | #params | download                                  |
+========================================================================+==============+===========+===========+============+============+=========+===========================================+
| `Kahn etal <https://arxiv.org/abs/1909.09116>`_                        | BiLSTM       | 14.00     | 37.02     | 14.85      | 39.95      | \-      |                                           |
+------------------------------------------------------------------------+--------------+-----------+-----------+------------+------------+---------+-------------------------------------------+
| `Laptev etal <https://arxiv.org/abs/2005.07157>`_                      | Transformer  | 10.3      | 24.0      | 11.2       | 24.9       | \-      |                                           |
+------------------------------------------------------------------------+--------------+-----------+-----------+------------+------------+---------+-------------------------------------------+
| `ESPnet <https://huggingface.co/pyf98/librispeech_100h_transformer>`__ | Transformer  |  8.1      | 20.2      |  8.4       | 20.5       | \-      |                                           |
+------------------------------------------------------------------------+--------------+-----------+-----------+------------+------------+---------+-------------------------------------------+
| `ESPnet <https://huggingface.co/pyf98/librispeech_100h_conformer>`__   | Conformer    |  6.3      | 17.4      |  6.5       | 17.3       | \-      |                                           |
+------------------------------------------------------------------------+--------------+-----------+-----------+------------+------------+---------+-------------------------------------------+
| JoeyS2T                                                                | Transformer  | 10.18     | 23.39     | 11.58      | 24.31      | 93M     | :joeynmt2:`librispeech100h.tar.gz` (948M) |
+------------------------------------------------------------------------+--------------+-----------+-----------+------------+------------+---------+-------------------------------------------+


LibriSpeech 960h
^^^^^^^^^^^^^^^^

+-----------------------------------------------------------------------------------------------+--------------+-----------+-----------+------------+------------+---------+-------------------------------------------+
| System                                                                                        | Architecture | dev-clean | dev-other | test-clean | test-other | #params | download                                  |
+===============================================================================================+==============+===========+===========+============+============+=========+===========================================+
| `Gulati etal <https://arxiv.org/abs/2005.08100>`_                                             | BiLSTM       |  1.9      |  4.4      |  2.1       |  4.9       | \-      | \-                                        |
+-----------------------------------------------------------------------------------------------+--------------+-----------+-----------+------------+------------+---------+-------------------------------------------+
| `ESPnet <https://github.com/espnet/espnet/tree/v.202207/egs2/librispeech/asr1#without-lm>`__  | Conformer    |  2.3      |  6.1      |  2.6       |  6.0       | \-      | \-                                        |
+-----------------------------------------------------------------------------------------------+--------------+-----------+-----------+------------+------------+---------+-------------------------------------------+
| `SpeechBrain <https://huggingface.co/speechbrain/asr-transformer-transformerlm-librispeech>`_ | Conformer    |  2.13     |  5.51     |  2.31      |  5.61      | 165M    | \-                                        |
+-----------------------------------------------------------------------------------------------+--------------+-----------+-----------+------------+------------+---------+-------------------------------------------+
| `fairseq S2T <https://huggingface.co/facebook/s2t-small-librispeech-asr>`_                    | Transformer  |  3.23     |  8.01     |  3.52      |  7.83      | 71M     | \-                                        |
+-----------------------------------------------------------------------------------------------+--------------+-----------+-----------+------------+------------+---------+-------------------------------------------+
| `fairseq wav2vec2 <https://huggingface.co/facebook/wav2vec2-base-960h>`_                      | Conformer    |  3.17     |  8.87     |  3.39      |  8.57      | 94M     | \-                                        |
+-----------------------------------------------------------------------------------------------+--------------+-----------+-----------+------------+------------+---------+-------------------------------------------+
| JoeyS2T                                                                                       | Transformer  | 10.18     | 23.39     | 11.58      | 24.31      | 102M    | :joeynmt2:`librispeech960h.tar.gz` (1.1G) |
+-----------------------------------------------------------------------------------------------+--------------+-----------+-----------+------------+------------+---------+-------------------------------------------+


MuST-C ASR pretraining
^^^^^^^^^^^^^^^^^^^^^^

+----------------------------------------------------------------------------------------+-------+-------+-------+------------+--------+---------+-------------------------------------+
| System                                                                                 | train | eval  | dev   | tst-COMMON | tst-HE | #params | download                            |
+========================================================================+===============+=======+=======+=======+============+========+=========+=====================================+
| `Gangi etal <https://cris.fbk.eu/retrieve/handle/11582/319654/29817/3045.pdf>`_        | v1    | v1    | \-    | 27.0       | \-     | \-      |                                     |
+----------------------------------------------------------------------------------------+-------+-------+-------+------------+--------+---------+-------------------------------------+
| `ESPnet <https://github.com/espnet/espnet/tree/v.202207/egs/must_c/asr1/RESULTS.md>`__ | v1    | v1    | \-    | 12.70      | \-     | \-      |                                     |
+----------------------------------------------------------------------------------------+-------+-------+-------+------------+--------+---------+-------------------------------------+
| :fairseq:`fairseq S2T <speech_to_text/docs/mustc_example.md>`                          | v1    | v1    | 13.07 | 12.72      | 10.93  | 29.5M   |                                     |
+----------------------------------------------------------------------------------------+-------+-------+-------+------------+--------+---------+-------------------------------------+
| :fairseq:`fairseq S2T <speech_to_text/docs/mustc_example.md>`                          | v1    | v2    |  9.11 | 11.88      | 10.43  | 29.5M   |                                     |
+----------------------------------------------------------------------------------------+-------+-------+-------+------------+--------+---------+-------------------------------------+
| JoeyS2T                                                                                | v2    | v1    | 18.09 | 18.66      | 14.97  | 96M     |                                     |
+----------------------------------------------------------------------------------------+-------+-------+-------+------------+--------+---------+-------------------------------------+
| JoeyS2T                                                                                | v2    | v2    |  9.77 | 12.51      | 10.73  | 96M     | :joeynmt2:`mustc_asr.tar.gz` (940M) |
+----------------------------------------------------------------------------------------+-------+-------+-------+------------+--------+---------+-------------------------------------+


MuST-C MT pretraining
^^^^^^^^^^^^^^^^^^^^^

+----------------------------------------------------------------------------------------+-------+-------+-------+------------+--------+---------+-------------------------------------+
| System                                                                                 | train | eval  | dev   | tst-COMMON | tst-HE | #params | download                            |
+========================================================================+===============+=======+=======+=======+============+========+=========+=====================================+
| `Gangi etal <https://cris.fbk.eu/retrieve/handle/11582/319654/29817/3045.pdf>`_        | v1    | v1    | \-    | 25.3       | \-     | \-      |                                     |
+----------------------------------------------------------------------------------------+-------+-------+-------+------------+--------+---------+-------------------------------------+
| `Zhang etal <https://aclanthology.org/2020.findings-emnlp.230/>`_                      | v1    | v1    | \-    | 29.69      | \-     | \-      |                                     |
+----------------------------------------------------------------------------------------+-------+-------+-------+------------+--------+---------+-------------------------------------+
| `ESPnet <https://github.com/espnet/espnet/tree/v.202207/egs/must_c/asr1/RESULTS.md>`__ | v1    | v1    | \-    | 27.63      | \-     | \-      |                                     |
+----------------------------------------------------------------------------------------+-------+-------+-------+------------+--------+---------+-------------------------------------+
| JoeyS2T                                                                                | v2    | v1    | 21.85 | 23.15      | 20.37  | 66.5M   |                                     |
+----------------------------------------------------------------------------------------+-------+-------+-------+------------+--------+---------+-------------------------------------+
| JoeyS2T                                                                                | v2    | v2    | 26.99 | 27.61      | 25.26  | 66.5M   | :joeynmt2:`mustc_mt.tar.gz` (729M)  |
+----------------------------------------------------------------------------------------+-------+-------+-------+------------+--------+---------+-------------------------------------+


MuST-C end-to-end ST
^^^^^^^^^^^^^^^^^^^^

+----------------------------------------------------------------------------------------+-------+-------+-------+------------+--------+---------+-------------------------------------+
| System                                                                                 | train | eval  | dev   | tst-COMMON | tst-HE | #params | download                            |
+========================================================================+===============+=======+=======+=======+============+========+=========+=====================================+
| `Gangi etal <https://cris.fbk.eu/retrieve/handle/11582/319654/29817/3045.pdf>`_        | v1    | v1    | \-    | 17.3       | \-     | \-      |                                     |
+----------------------------------------------------------------------------------------+-------+-------+-------+------------+--------+---------+-------------------------------------+
| `Zhang etal <https://aclanthology.org/2020.findings-emnlp.230/>`_                      | v1    | v1    | \-    | 20.67      | \-     | \-      |                                     |
+----------------------------------------------------------------------------------------+-------+-------+-------+------------+--------+---------+-------------------------------------+
| `ESPnet <https://github.com/espnet/espnet/tree/v.202207/egs/must_c/st1/RESULTS.md>`__  | v1    | v1    | \-    | 22.91      | \-     | \-      |                                     |
+----------------------------------------------------------------------------------------+-------+-------+-------+------------+--------+---------+-------------------------------------+
| :fairseq:`fairseq S2T <speech_to_text/docs/mustc_example.md>`                          | v1    | v2    | 22.05 | 22.70      | 21.70  | 31M     |                                     |
+----------------------------------------------------------------------------------------+-------+-------+-------+------------+--------+---------+-------------------------------------+
| JoeyS2T                                                                                | v2    | v1    | 21.06 | 20.92      | 21.78  | 96M     |                                     |
+----------------------------------------------------------------------------------------+-------+-------+-------+------------+--------+---------+-------------------------------------+
| JoeyS2T                                                                                | v2    | v2    | 24.26 | 23.86      | 23.86  | 96M     | :joeynmt2:`mustc_st.tar.gz` (952M)  |
+----------------------------------------------------------------------------------------+-------+-------+-------+------------+--------+---------+-------------------------------------+

sacrebleu signature: `nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.1.0`

.. note::

    For MuST-C, we trained our model on the English-German subset of version 2, and evaluated the model both on version 1 and version 2 ``tst-COMMON``, ``and tst-HE splits``. See :notebooks:`benchmarks.ipynb` to replicate these results.


JoeyNMT v2.x
------------

IWSLT14 de/en/fr multilingual
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^


We trained this multilingual model with JoeyNMT v2.3.0 using DDP.

+-----------+--------------+---------------+-------+-------+---------+--------------------------------------------------------------------+
| Direction | Architecture | Tokenizer     | dev   | test  | #params | download                                                           |
+===========+==============+===============+=======+=======+=========+====================================================================+
| en->de    | Transformer  | sentencepiece |    \- | 28.88 | 200M    | `iwslt14_prompt <https://huggingface.co/may-ohta/iwslt14_prompt>`_ |
+-----------+              +               +-------+-------+         +                                                                    +
| de->en    |              |               |    \- | 35.28 |         |                                                                    |
+-----------+              +               +-------+-------+         +                                                                    +
| en->fr    |              |               |    \- | 38.86 |         |                                                                    |
+-----------+              +               +-------+-------+         +                                                                    +
| fr->en    |              |               |    \- | 40.35 |         |                                                                    |
+-----------+--------------+---------------+-------+-------+---------+--------------------------------------------------------------------+

sacrebleu signature: `nrefs:1|case:lc|eff:no|tok:13a|smooth:exp|version:2.4.0`


WMT14 ende / deen
^^^^^^^^^^^^^^^^^

We trained the models with JoeyNMT v2.1.0 from scratch.

cf) `wmt14 deen leaderboard <https://paperswithcode.com/sota/machine-translation-on-wmt2014-german-english>`_ in paperswithcode

+-----------+--------------+---------------+-------+-------+---------+----------------------------------------------------------------------------------------------------+
| Direction | Architecture | Tokenizer     | dev   | test  | #params | download                                                                                           |
+===========+==============+===============+=======+=======+=========+====================================================================================================+
| en->de    | Transformer  | sentencepiece | 24.36 | 24.38 | 60.5M   | `wmt14_ende.tar.gz <https://cl.uni-heidelberg.de/statnlpgroup/joeynmt2/wmt14_ende.tar.gz>`_ (766M) |
+-----------+--------------+---------------+-------+-------+---------+----------------------------------------------------------------------------------------------------+
| de->en    | Transformer  | sentencepiece | 30.60 | 30.51 | 60.5M   | `wmt14_deen.tar.gz <https://cl.uni-heidelberg.de/statnlpgroup/joeynmt2/wmt14_deen.tar.gz>`_ (766M) |
+-----------+--------------+---------------+-------+-------+---------+----------------------------------------------------------------------------------------------------+

sacrebleu signature: `nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0`


JoeyNMT v1.x
------------

.. warning::

    The following models are trained with JoeynNMT v1.x, and decoded with Joey NMT v2.0. 
    See ``config_v1.yaml`` and ``config_v2.yaml`` in the linked tar.gz, respectively.
    Joey NMT v1.x benchmarks are archived `here <https://github.com/joeynmt/joeynmt/blob/main/docs/benchmarks_v1.md>`__.


IWSLT14 deen
^^^^^^^^^^^^

Pre-processing with Moses decoder tools as in this :scripts:`script <get_iwslt14_bpe.sh>`.

+-----------+--------------+-------------+-------+-------+---------+----------------------------------------------------------------------------------------------------------------------------------------+
| Direction | Architecture | Tokenizer   | dev   | test  | #params | download                                                                                                                               |
+===========+==============+=============+=======+=======+=========+========================================================================================================================================+
| de->en    | RNN          | subword-nmt | 31.77 | 30.74 | 61M     | `rnn_iwslt14_deen_bpe.tar.gz <https://cl.uni-heidelberg.de/statnlpgroup/joeynmt2/rnn_iwslt14_deen_bpe.tar.gz>`_ (672M)                 |
+-----------+--------------+-------------+-------+-------+---------+----------------------------------------------------------------------------------------------------------------------------------------+
| de->en    | Transformer  | subword-nmt | 34.53 | 33.73 | 19M     | `transformer_iwslt14_deen_bpe.tar.gz <https://cl.uni-heidelberg.de/statnlpgroup/joeynmt2/transformer_iwslt14_deen_bpe.tar.gz>`_ (221M) |
+-----------+--------------+-------------+-------+-------+---------+----------------------------------------------------------------------------------------------------------------------------------------+

sacrebleu signature: `nrefs:1|case:lc|eff:no|tok:13a|smooth:exp|version:2.0.0`

.. note::

    For interactive translate mode, you should specify ``pretokenizer: "moses"`` in both src's and trg's ``tokenizer_cfg``,
    so that you can input raw sentences. Then ``MosesTokenizer`` and ``MosesDetokenizer`` will be applied internally.
    For test mode, we used the preprocessed texts as input and set ``pretokenizer: "none"`` in the config.


Masakhane JW300 afen / enaf
^^^^^^^^^^^^^^^^^^^^^^^^^^^

We picked the pretrained models and configs (bpe codes file etc.) from `masakhane.io <https://github.com/masakhane-io/masakhane-mt>`_.

+-----------+--------------+-------------+-------+-------+---------+----------------------------------------------------------------------------------------------------------------------------+
| Direction | Architecture | Tokenizer   | dev   | test  | #params | download                                                                                                                   |
+===========+==============+=============+=======+=======+=========+============================================================================================================================+
| af->en    | Transformer  | subword-nmt | \-    | 57.70 | 46M     | `transformer_jw300_afen.tar.gz <https://cl.uni-heidelberg.de/statnlpgroup/joeynmt2/transformer_jw300_afen.tar.gz>`_ (525M) |
+-----------+--------------+-------------+-------+-------+---------+----------------------------------------------------------------------------------------------------------------------------+
| en->af    | Transformer  | subword-nmt | 47.24 | 47.31 | 24M     | `transformer_jw300_enaf.tar.gz <https://cl.uni-heidelberg.de/statnlpgroup/joeynmt2/transformer_jw300_enaf.tar.gz>`_ (285M) |
+-----------+--------------+-------------+-------+-------+---------+----------------------------------------------------------------------------------------------------------------------------+

sacrebleu signature: `nrefs:1|case:mixed|eff:no|tok:intl|smooth:exp|version:2.0.0`


JParaCrawl enja / jaen
^^^^^^^^^^^^^^^^^^^^^^

For training, we split JparaCrawl v2 into train and dev set and trained a model on them.
Please check the preprocessing script `here <https://github.com/joeynmt/joeynmt/blob/v2.2/scripts/get_jparacrawl.sh>`__.
We tested then on `kftt <http://www.phontron.com/kftt/>`_ test set and `wmt20 <https://data.statmt.org/wmt20/translation-task/>`_ test set, respectively.

+-----------+--------------+---------------+-------+-------+---------+---------------------------------------------------------------------------------------------------------------+
| Direction | Architecture | Tokenizer     | kftt  | wmt20 | #params | download                                                                                                      |
+===========+==============+===============+=======+=======+=========+===============================================================================================================+
| ja->en    | Transformer  | sentencepiece | 17.66 | 14.31 | 225M    | `jparacrawl_enja.tar.gz <https://cl.uni-heidelberg.de/statnlpgroup/joeynmt2/jparacrawl_enja.tar.gz>`_ (2.3GB) |
+-----------+--------------+---------------+-------+-------+---------+---------------------------------------------------------------------------------------------------------------+
| en->ja    | Transformer  | sentencepiece | 14.97 | 11.49 | 221M    | `jparacrawl_jaen.tar.gz <https://cl.uni-heidelberg.de/statnlpgroup/joeynmt2/jparacrawl_jaen.tar.gz>`_ (2.2GB) |
+-----------+--------------+---------------+-------+-------+---------+---------------------------------------------------------------------------------------------------------------+

sacrebleu signature:
    - en->ja: `nrefs:1|case:mixed|eff:no|tok:ja-mecab-0.996-IPA|smooth:exp|version:2.0.0`
    - ja->en: `nrefs:1|case:mixed|eff:no|tok:intl|smooth:exp|version:2.0.0`

(Note: In wmt20 test set, `newstest2020-enja` has 1000 examples, `newstest2020-jaen` has 993 examples.)
